# ðŸš€ My Hands-On LLM Learning Journey

This repository tracks my hands-on learning journey with Local Large Language Models (LLMs), focusing on practical implementation and experimentation. Each phase builds upon the previous one, creating a comprehensive understanding of LLM development from environment setup to advanced applications.

## ðŸ§­ Phases

### âœ… [Phase 0](./phase0/)
> *Goal:* Set up robust development environment (WSL2 + Python + CUDA + PyTorch GPU)
- Complete WSL2 configuration with GPU support
- Python 3.11 environment setup
- VScode IDE setup + github, copilot, WSL venv
- CUDA toolkit and PyTorch integration
- Jupyter notebook first tests
- Basic GPU validation tests via shell

### âœ… [Phase 1](./phase1/)
> *Goal:* Local LLM experimentation and integration
- Interactive notebooks and examples to get started with AI Python
- API integrations for various endpoints, both local and remote (eg local to colab)
- Local model deployment (Ollama and Oobabooga - both tried in Windows 10 and inside WSL 2 (ubuntu))
- Multiple model architectures
- Performance analysis and optimization (It turned out the Windows Ollama is fastest of my 4 local alternative setups)

MY NEXT STEPs PLAN:

### âœ… [Phase 2](./phase2/)
> *Goal:* Local RAG experimentation and integration
- Trying out multiple local RAG architectures

### âœ… [Phase 3](./phase3/)
> *Goal:* AI safety tools experimentation
- Trying out multiple AI safety testing frameworks and approaches

### ðŸ”œ Future Phases
> Planning in progress...

## ðŸ›  Key Features

- **WSL2 + GPU Integration**: Optimized for Windows development with Linux tools
- **Local LLM Focus**: Run and experiment with models on your own hardware
- **Comprehensive Documentation**: Clear setup instructions and examples
- **Performance Analysis**: Benchmarking and optimization tools
- **Interactive Learning**: Jupyter notebooks for hands-on experimentation

## ðŸ“Œ Why This Exists

This repository serves as both a learning journal and a practical reference for LLM development. It demonstrates:
- Setting up a robust development environment
- Working with local LLM deployments
- Understanding model behavior and performance
- Practical implementation patterns and best practices

The goal is to provide a clear path from initial setup to advanced LLM applications, with each phase building upon previous knowledge.M Learning Journey

This repository tracks my hands-on learning with Python, GitHub, WSL, and Large Language Models (LLMs). Each phase is a self-contained learning project.