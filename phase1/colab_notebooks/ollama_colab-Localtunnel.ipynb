{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsVRADXr0XQd"
   },
   "source": [
    "I never got it to work – in the Python view of VSCode, I kept getting either a tunnel error or the ollama server stopped responding. In the end, the Colab GPU daily time ran out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_z3Eer-NQ2E"
   },
   "source": [
    "# Running open AI models for free in under 10 minutes with a Google Colab and no extra accounts\n",
    "https://cubed.run/blog/running-open-ai-models-for-free-in-under-10-minutes-with-a-google-colab-and-no-extra-accounts-yes-4d38b59f0153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29929,
     "status": "ok",
     "timestamp": 1750766476652,
     "user": {
      "displayName": "Kristjan-Hans Sillmann",
      "userId": "09734310897857365932"
     },
     "user_tz": -180
    },
    "id": "_dx8LOCnz9Zw",
    "outputId": "aa3ba24d-9763-4045-c317-07cd3dc7185f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1750766485857,
     "user": {
      "displayName": "Kristjan-Hans Sillmann",
      "userId": "09734310897857365932"
     },
     "user_tz": -180
    },
    "id": "_kEeSF8jz0AE",
    "outputId": "d5d283f5-47fc-4c44-87ca-c73e1655617e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /content/drive/MyDrive/Colab Projects\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/Colab Projects\")\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44124,
     "status": "ok",
     "timestamp": 1750766232228,
     "user": {
      "displayName": "Kristjan-Hans Sillmann",
      "userId": "09734310897857365932"
     },
     "user_tz": -180
    },
    "id": "3s17tEEtNGRB",
    "outputId": "7f58911f-5dc7-4b71-b988-4f4403b25707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n"
     ]
    }
   ],
   "source": [
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "# Run Ollama service in the background\n",
    "!ollama serve &>/dev/null&\n",
    "# Download LLaVA model (13b parameters, 1.6ver)\n",
    "!ollama pull llava:7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1750766242896,
     "user": {
      "displayName": "Kristjan-Hans Sillmann",
      "userId": "09734310897857365932"
     },
     "user_tz": -180
    },
    "id": "3K9vl3uyNa0z",
    "outputId": "62e20c68-8f94-46d4-d04b-dddb1aeba880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         ID              SIZE      MODIFIED       \n",
      "llava:13b    0d0eb4d7f485    8.0 GB    10 minutes ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1750765608456,
     "user": {
      "displayName": "Kristjan-Hans Sillmann",
      "userId": "09734310897857365932"
     },
     "user_tz": -180
    },
    "id": "MztWPklwSfyn",
    "outputId": "bfeaa973-cd61-4233-9505-e986111ce716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.105.121.87"
     ]
    }
   ],
   "source": [
    "# Check Colab's instance public IP address.\n",
    "!curl ifconfig.me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 566375,
     "status": "ok",
     "timestamp": 1750766174950,
     "user": {
      "displayName": "Kristjan-Hans Sillmann",
      "userId": "09734310897857365932"
     },
     "user_tz": -180
    },
    "id": "uJYWo_SvTYEe",
    "outputId": "a4b7bf33-aa6a-4caf-8542-ace1fd8716f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
      "added 22 packages in 2s\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K3 packages are looking for funding\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://free-lines-camp.loca.lt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Download and install Localtunnel\n",
    "!npm install localtunnel\n",
    "# Run Localtunnel and redirect to localhost:11434\n",
    "!npx localtunnel -p 11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RLZGS8BTgAw"
   },
   "outputs": [],
   "source": [
    "!wget -q -O - https://hip-waves-doubt.loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WezTquU4WgrO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNt8el5zADqoZm5i75JS18F",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1XzYkMCQmg4PcBZkeyX6eXFneSDndpKtn",
     "timestamp": 1750766493214
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
