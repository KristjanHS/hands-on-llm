"""
# eeldab kohaliku ollama serveri olemasolu wsl 2 ubuntu sees, mis töötab pordil 11434.
curl -fsSL https://ollama.com/install.sh | sh
ollama --version

ollama run mistral
ctrl +d to exit  
or 
ollama run mistral &>/dev/null&
# to run it in background!
  Purpose: Starts an interactive session with the Mistral model directly in your terminal.
  Behavior: Temporarily initiates the Ollama server to facilitate this session.
  Use Case: Ideal for quick tests or one-off interactions where you don't need persistent access.

ollama serve
or 
ollama serve &>/dev/null&
# to run it in background!
  Purpose: Launches the Ollama server as a standalone service, continuously listening for API requests.
  Behavior: Keeps the server running independently, allowing multiple clients or applications to interact with it concurrently.
  Use Case: Essential when integrating Ollama into applications, using it with tools like LangChain, or exposing it over a network.
  Test it via linux shell: curl http://localhost:11434
"""
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="mistral", base_url="http://localhost:11434")

def generate_text(prompt: str) -> str:
    """Generate text using the Mistral model."""
    response = llm.invoke(prompt)
    return response.strip()

# Example usage
print("Generating text with Mistral model...\n")
print("Prompt: Write the summary of the book 'The Alchemist', by Paulo Coelho\n")
print("Response:\n")
for text in llm.invoke("Write the summary of the book 'The Alchemist', by Paulo Coelho"):
    print(text, end="", flush=True)