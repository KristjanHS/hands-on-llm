"""
LEARNING todo:
* run local oobabooga server on WSL, point base_url in the code to localhost:oobabooga_port

# Running this python code inside WSL 2 assumes 
#   that local LLM server (ollama or oobabooga) is also running inside WSL2 !
#
# ollama is a great alternative to oobabooga, especially for those who prefer a simpler setup and 
# don't need the full web interface that oobabooga provides.
"""
from langchain_ollama import OllamaLLM

# OllamaLLM wrapper does not work with oobabooga models, so for oobabooga, you need to use the oobabooga wrapper.
llm = OllamaLLM(model="SmolLM-135M", base_url="http://localhost:5000")
# oobabooga:  http://127.0.0.1:5000   model = "SmolLM-135M"
# ollama:     http://localhost:11434   model="mistral" 

def generate_text(prompt: str) -> str:
    """Generate text using the model."""
    response = llm.invoke(prompt)
    return response.strip()

# Example usage
print("Generating text with model...\n")
print("Prompt: Write the summary of the book 'The Alchemist', by Paulo Coelho\n")
print("Response:\n")
for text in llm.invoke("Write the summary of the book 'The Alchemist', by Paulo Coelho"):
    print(text, end="", flush=True)