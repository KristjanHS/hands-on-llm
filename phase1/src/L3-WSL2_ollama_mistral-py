"""
LEARNING todo:
* run local oobabooga server on WSL, point base_url in the code to localhost:THE_RIGHT_PORT

* ALREADY DONE, WORKS: run local ollama server on WSL, point base_url in the code to localhost:THE_RIGHT_PORT

# Running this python code inside WSL 2 assumes 
#   that local LLM server (ollama or oobabooga) is also running inside WSL2 !
#
# ollama is a great alternative to oobabooga, especially for those who prefer a simpler setup and 
# don't need the full web interface that oobabooga provides.

* Ollama install:
curl -fsSL https://ollama.com/install.sh | sh
ollama --version

ollama run mistral
# ctrl +d to exit  
# or 
ollama run mistral &>/dev/null&
# to run it in background!
  Purpose: Starts an interactive session with the Mistral model directly in your terminal.
  Behavior: Temporarily initiates the Ollama server to facilitate this session.
  Use Case: Ideal for quick tests or one-off interactions where you don't need persistent access.

ollama serve
# or 
ollama serve &>/dev/null&
# to run it in background!
  Purpose: Launches the Ollama server as a standalone service, continuously listening for API requests.
  Behavior: Keeps the server running independently, allowing multiple clients or applications to interact with it concurrently.
  Use Case: Essential when integrating Ollama into applications, using it with tools like LangChain, or exposing it over a network.
  Test it via linux shell: curl http://localhost:11434

# Oobabooga WSL 2 install with miniconda:
curl -sL https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o Miniconda3.sh
bash Miniconda3.sh
source ~/.bashrc
git clone https://github.com/oobabooga/text-generation-webui.git
cd text-generation-webui
conda create -n textgen python=3.10.9 -y
# because oobabooga is tested / verified to be stable on this python version!
conda activate textgen
pip install -r requirements.txt
# Place the Model: place model files into the models directory within text-generation-webui
# ~/text-generation-webui/models 
python server.py --api 
# This will launch the web interface, typically accessible at http://localhost:7860
# optional: --chat 
# optional: --auto-launch
# selle asemel vÃµib kasutada shell scripti:
# Create a shell script (e.g., start_oobabooga.sh) in your project directory:
#!/bin/bash
source venv/bin/activate
python server.py --chat --api
Make the script executable:
chmod +x start_oobabooga.sh
Run the script in a VSCode terminal:
./start_oobabooga.sh

"""
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="mistral", base_url="http://localhost:11434")

def generate_text(prompt: str) -> str:
    """Generate text using the Mistral model."""
    response = llm.invoke(prompt)
    return response.strip()

# Example usage
print("Generating text with Mistral model...\n")
print("Prompt: Write the summary of the book 'The Alchemist', by Paulo Coelho\n")
print("Response:\n")
for text in llm.invoke("Write the summary of the book 'The Alchemist', by Paulo Coelho"):
    print(text, end="", flush=True)